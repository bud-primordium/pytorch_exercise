"""
MÃ¼ller-BrownåŠ¿èƒ½ç¥ç»ç½‘ç»œå›å½’æ¨¡å‹å¯¹æ¯”
"""

# å¯¼å…¥å¿…è¦åº“
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from typing import Tuple, Dict, Any
import time
import platform
import os
import datetime
import json

# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆæ ¹æ®æ“ä½œç³»ç»Ÿè®¾ç½®åˆé€‚çš„å­—ä½“ï¼‰
if platform.system() == "Windows":
    plt.rcParams["font.sans-serif"] = ["Microsoft YaHei", "SimHei"]  # å¾®è½¯é›…é»‘å’Œé»‘ä½“
elif platform.system() == "Darwin":  # macOS
    plt.rcParams["font.sans-serif"] = ["Arial Unicode MS", "PingFang SC", "Heiti SC"]
else:  # Linux
    plt.rcParams["font.sans-serif"] = ["WenQuanYi Micro Hei", "DejaVu Sans"]

plt.rcParams["axes.unicode_minus"] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·


# ç”Ÿæˆæ—¶é—´æˆ³å­—ç¬¦ä¸²ï¼Œç”¨äºæ–‡ä»¶å‘½å
def get_timestamp() -> str:
    """ç”Ÿæˆæ ¼å¼åŒ–çš„æ—¶é—´æˆ³å­—ç¬¦ä¸²"""
    return datetime.datetime.now().strftime("%Y%m%d_%H%M%S")


# åˆ›å»ºç»“æœä¿å­˜ç›®å½•
def create_result_dir() -> str:
    """åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„ç»“æœç›®å½•"""
    timestamp = get_timestamp()
    result_dir = f"results_{timestamp}"
    os.makedirs(result_dir, exist_ok=True)
    os.makedirs(os.path.join(result_dir, "models"), exist_ok=True)
    os.makedirs(os.path.join(result_dir, "figures"), exist_ok=True)
    print(f"åˆ›å»ºç»“æœç›®å½•: {result_dir}")
    return result_dir


# è·å–æœ€ä½³å¯ç”¨è®¾å¤‡
def get_device(prefer: str = "cuda") -> torch.device:
    """è‡ªåŠ¨é€‰æ‹©å¯ç”¨è®¾å¤‡ï¼Œæ”¯æŒç°ä»£åŠ é€Ÿåç«¯"""
    device_priority = [prefer, "mps", "cpu"]  # macOS Metal åŠ é€Ÿ

    for device in device_priority:
        if device == "cuda" and torch.cuda.is_available():
            # æ£€æµ‹CUDAè®¡ç®—èƒ½åŠ›
            if hasattr(torch.cuda, "get_device_capability"):
                capability = torch.cuda.get_device_capability()
                print(f"ğŸš€ æ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œè®¡ç®—èƒ½åŠ›: {capability[0]}.{capability[1]}")
            return torch.device(device)
        if (
            device == "mps"
            and hasattr(torch.backends, "mps")
            and torch.backends.mps.is_available()
        ):
            print("ğŸ å¯ä½¿ç”¨Apple Metal Performance Shaders (MPS) åŠ é€Ÿ")
            return torch.device(device)
    return torch.device("cpu")


# è®¾ç½®è®¡ç®—è®¾å¤‡
device = get_device()
print(
    f"ğŸ¤– æ­£åœ¨ä½¿ç”¨åŠ é€Ÿè®¾å¤‡: {device.type.upper()}"
    if device.type != "cpu"
    else "ğŸ–¥ï¸ ä½¿ç”¨ CPU è¿è¡Œ"
)


# %% [1] å®šä¹‰MÃ¼ller-BrownåŠ¿èƒ½å‡½æ•°
def muller_brown_potential(x1: np.ndarray, x2: np.ndarray) -> np.ndarray:
    """
    è®¡ç®—MÃ¼ller-BrownåŠ¿èƒ½å€¼ï¼ˆè‡ªåŠ¨å¹¿æ’­æœºåˆ¶ï¼‰

    å‚æ•°ï¼š
        x1: x1åæ ‡å€¼æ•°ç»„ï¼Œå½¢çŠ¶(N,)
        x2: x2åæ ‡å€¼æ•°ç»„ï¼Œå½¢çŠ¶(N,)

    è¿”å›ï¼š
        U: åŠ¿èƒ½å€¼æ•°ç»„ï¼Œå½¢çŠ¶(N,)
    """
    s = 0.05
    A = np.array([-200, -100, -170, 15], dtype=np.float32)  # æŒ¯å¹…ç³»æ•°
    alpha = np.array([-1, -1, -6.5, 0.7], dtype=np.float32)  # äºŒæ¬¡é¡¹å‚æ•°Î±
    beta = np.array([0, 0, 11, 0.6], dtype=np.float32)  # äºŒæ¬¡é¡¹å‚æ•°Î²
    gamma = np.array([-10, -10, -6.5, 0.7], dtype=np.float32)  # äºŒæ¬¡é¡¹å‚æ•°Î³
    a = np.array([1, 0, -0.5, -1], dtype=np.float32)  # ä¸­å¿ƒåæ ‡a
    b = np.array([0, 0.5, 1.5, 1], dtype=np.float32)  # ä¸­å¿ƒåæ ‡b

    x1 = np.asarray(x1, dtype=np.float32)
    x2 = np.asarray(x2, dtype=np.float32)
    x1 = x1.reshape(-1, 1)
    x2 = x2.reshape(-1, 1)

    dx1 = x1 - a
    dx2 = x2 - b
    exponents = alpha * dx1**2 + beta * dx1 * dx2 + gamma * dx2**2
    U = s * np.sum(A * np.exp(exponents), axis=1)
    return np.minimum(U, 9.0)


# %% [2] å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹
class MLP(nn.Module):
    """ç®€å•å¤šå±‚æ„ŸçŸ¥æœº"""

    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(2, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.layers(x)


class StdNet(nn.Module):
    """å¸¦æ ‡å‡†åŒ–å±‚çš„ç¥ç»ç½‘ç»œ"""

    def __init__(self, mean: np.ndarray, std: np.ndarray):
        super().__init__()
        self.register_buffer("mean", torch.tensor(mean, dtype=torch.float32))
        self.register_buffer("std", torch.tensor(std, dtype=torch.float32))

        # æ¢å¤åˆ°åŸå§‹æ¶æ„ (2-256-128-1)
        self.fc1 = nn.Linear(2, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 1)
        self.dropout = nn.Dropout(0.1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = (x - self.mean) / self.std
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        return self.fc3(x)


class ResNet(nn.Module):
    """å¸¦æ®‹å·®è¿æ¥çš„ç¥ç»ç½‘ç»œ"""

    def __init__(self, mean: np.ndarray = None, std: np.ndarray = None):
        super().__init__()

        # æ˜¯å¦ä½¿ç”¨æ ‡å‡†åŒ–
        self.use_standardization = mean is not None and std is not None
        if self.use_standardization:
            self.register_buffer("mean", torch.tensor(mean, dtype=torch.float32))
            self.register_buffer("std", torch.tensor(std, dtype=torch.float32))

        # è¾“å…¥å¤„ç†
        self.input_layer = nn.Linear(2, 128)

        # æ®‹å·®å—
        self.res_block1 = self._make_res_block(128, 128)
        self.res_block2 = self._make_res_block(128, 128)

        # è¾“å‡ºå±‚
        self.output_layer = nn.Linear(128, 1)

    def _make_res_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Linear(in_channels, out_channels),
            nn.ReLU(),
            nn.Linear(out_channels, out_channels),
            nn.Dropout(0.1),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # æ ‡å‡†åŒ–è¾“å…¥
        if self.use_standardization:
            x = (x - self.mean) / self.std

        # è¾“å…¥å±‚
        x = F.relu(self.input_layer(x))

        # æ®‹å·®å—1
        identity = x
        out = self.res_block1(x)
        out += identity  # æ®‹å·®è¿æ¥
        out = F.relu(out)

        # æ®‹å·®å—2
        identity = out
        out = self.res_block2(out)
        out += identity  # æ®‹å·®è¿æ¥
        out = F.relu(out)

        # è¾“å‡ºå±‚
        return self.output_layer(out)


# %% [3] æ•°æ®å‡†å¤‡å‡½æ•°
def prepare_data(
    data_file: str = "train_data.txt",
    test_size: float = 0.2,
    random_state: int = 42,
    augment: bool = False,
    n_augment: int = 0,
) -> Tuple[DataLoader, DataLoader, Dict[str, Any]]:
    """
    å‡†å¤‡è®­ç»ƒæ•°æ®

    å‚æ•°ï¼š
        data_file: è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
        test_size: éªŒè¯é›†æ¯”ä¾‹
        random_state: éšæœºç§å­
        augment: æ˜¯å¦å¢å¹¿æ•°æ®
        n_augment: å¢å¹¿æ•°æ®é‡

    è¿”å›ï¼š
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        data_info: æ•°æ®ä¿¡æ¯å­—å…¸
    """
    # ä»æ–‡ä»¶åŠ è½½æ•°æ®
    if os.path.exists(data_file):
        print(f"ä»æ–‡ä»¶ {data_file} åŠ è½½æ•°æ®...")
        data = np.loadtxt(data_file, dtype=np.float32)
        X = data[:, :2]  # å‰ä¸¤åˆ—æ˜¯åæ ‡
        U = data[:, 2]  # ç¬¬ä¸‰åˆ—æ˜¯åŠ¿èƒ½å€¼
    else:
        print(f"æœªæ‰¾åˆ°æ–‡ä»¶ {data_file}ï¼Œç”Ÿæˆéšæœºæ•°æ®...")
        # ç”Ÿæˆæ•°æ® (æ³¨æ„: x1èŒƒå›´ä¸º[-1.5, 1.5]ï¼Œx2èŒƒå›´ä¸º[-0.5, 2.0])
        x1 = np.random.uniform(low=-1.5, high=1.5, size=500).astype(np.float32)
        x2 = np.random.uniform(low=-0.5, high=2.0, size=500).astype(np.float32)
        X = np.stack([x1, x2], axis=1)
        U = muller_brown_potential(x1, x2)

    # æ•°æ®å¢å¹¿ - ä½¿ç”¨æ›´æ™ºèƒ½çš„é‡‡æ ·ç­–ç•¥
    if augment and n_augment > 0:
        print(f"å¢å¹¿æ•°æ®ï¼Œé¢å¤–æ·»åŠ  {n_augment} ä¸ªç‚¹...")

        # åˆ†æåŸå§‹æ•°æ®åˆ†å¸ƒ
        x1_min, x1_max = X[:, 0].min(), X[:, 0].max()
        x2_min, x2_max = X[:, 1].min(), X[:, 1].max()

        # æˆ˜ç•¥æ€§æ•°æ®å¢å¹¿ï¼Œå…³æ³¨ä»¥ä¸‹åŒºåŸŸï¼š
        # 1. æ•°æ®ç¨€ç–åŒºåŸŸ - ä½¿ç”¨å‡åŒ€ç½‘æ ¼é‡‡æ ·
        # 2. æ¢¯åº¦å¤§çš„åŒºåŸŸ - åŠ¿èƒ½å˜åŒ–å‰§çƒˆçš„åœ°æ–¹éœ€è¦æ›´å¯†é›†é‡‡æ ·
        # 3. å†³ç­–è¾¹ç•ŒåŒºåŸŸ - æ¨¡å‹é€šå¸¸åœ¨è¿™äº›åŒºåŸŸè¡¨ç°æœ€å·®

        # 1. å‡åŒ€ç½‘æ ¼é‡‡æ · (40%)
        n_grid = int(n_augment * 0.4)
        x1_grid = np.linspace(-1.5, 1.5, int(np.sqrt(n_grid))).astype(np.float32)
        x2_grid = np.linspace(-0.5, 2.0, int(np.sqrt(n_grid))).astype(np.float32)
        X1, X2 = np.meshgrid(x1_grid, x2_grid)
        X_grid = np.stack([X1.flatten(), X2.flatten()], axis=1)

        # åªä¿ç•™n_gridä¸ªç‚¹
        if len(X_grid) > n_grid:
            indices = np.random.choice(len(X_grid), n_grid, replace=False)
            X_grid = X_grid[indices]

        # 2. è¾¹ç¼˜åŒºåŸŸé‡‡æ · (30%) - ä¿®å¤é•¿åº¦ä¸åŒ¹é…é—®é¢˜
        n_edge = int(n_augment * 0.3)
        # ä¸ºå››ä¸ªåŒºåŸŸæ˜ç¡®åˆ†é…ç›¸ç­‰æ•°é‡çš„ç‚¹
        n_per_region = n_edge // 4

        # x1è¾¹ç¼˜åŒºåŸŸ
        x1_edge_low = np.random.uniform(-1.5, -1.2, size=n_per_region).astype(
            np.float32
        )
        x1_edge_high = np.random.uniform(1.2, 1.5, size=n_per_region).astype(np.float32)
        x2_for_low_x1 = np.random.uniform(-0.5, 2.0, size=n_per_region).astype(
            np.float32
        )
        x2_for_high_x1 = np.random.uniform(-0.5, 2.0, size=n_per_region).astype(
            np.float32
        )

        # x2è¾¹ç¼˜åŒºåŸŸ
        x2_edge_low = np.random.uniform(-0.5, -0.2, size=n_per_region).astype(
            np.float32
        )
        x2_edge_high = np.random.uniform(1.7, 2.0, size=n_per_region).astype(np.float32)
        x1_for_low_x2 = np.random.uniform(-1.5, 1.5, size=n_per_region).astype(
            np.float32
        )
        x1_for_high_x2 = np.random.uniform(-1.5, 1.5, size=n_per_region).astype(
            np.float32
        )

        # åˆå¹¶å››ä¸ªåŒºåŸŸçš„ç‚¹
        X_edges = np.vstack(
            [
                np.column_stack([x1_edge_low, x2_for_low_x1]),  # ä½x1è¾¹ç¼˜
                np.column_stack([x1_edge_high, x2_for_high_x1]),  # é«˜x1è¾¹ç¼˜
                np.column_stack([x1_for_low_x2, x2_edge_low]),  # ä½x2è¾¹ç¼˜
                np.column_stack([x1_for_high_x2, x2_edge_high]),  # é«˜x2è¾¹ç¼˜
            ]
        )

        # 3. éšæœºé‡‡æ · (å‰©ä½™30%)
        n_random = n_augment - len(X_grid) - len(X_edges)
        x1_random = np.random.uniform(-1.5, 1.5, size=n_random).astype(np.float32)
        x2_random = np.random.uniform(-0.5, 2.0, size=n_random).astype(np.float32)
        X_random = np.column_stack([x1_random, x2_random])

        # åˆå¹¶æ‰€æœ‰å¢å¹¿æ•°æ®
        X_aug = np.vstack([X_grid, X_edges, X_random])

        # è®¡ç®—å¢å¹¿ç‚¹çš„åŠ¿èƒ½å€¼
        U_aug = muller_brown_potential(X_aug[:, 0], X_aug[:, 1])

        # åˆå¹¶åŸå§‹æ•°æ®å’Œå¢å¹¿æ•°æ®
        X = np.vstack([X, X_aug])
        U = np.hstack([U, U_aug])

        print(f"æ•°æ®å¢å¹¿åæ€»æ ·æœ¬æ•°: {len(X)}")

    # æ•°æ®é¢„å¤„ç†
    X_train, X_val, y_train, y_val = train_test_split(
        X, U, test_size=test_size, random_state=random_state
    )

    # è®¡ç®—æ ‡å‡†åŒ–å‚æ•°
    mean = X_train.mean(axis=0)
    std = X_train.std(axis=0)

    # è½¬æ¢ä¸ºå¼ é‡
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        TensorDataset(X_train_tensor, y_train_tensor), batch_size=64, shuffle=True
    )
    val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=64)

    return (
        train_loader,
        val_loader,
        {
            "mean": mean,
            "std": std,
            "X_train": X_train,
            "y_train": y_train,
            "X_val": X_val,
            "y_val": y_val,
        },
    )


# %% [4] è®­ç»ƒå‡½æ•°
def train_model(
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    criterion: nn.Module,
    optimizer: torch.optim.Optimizer,
    scheduler: torch.optim.lr_scheduler._LRScheduler = None,
    num_epochs: int = 1000,
    patience: int = 10,
    device: str = device,
) -> Tuple[nn.Module, float, Dict[str, list]]:
    """
    è®­ç»ƒæ¨¡å‹

    å‚æ•°ï¼š
        model: ç¥ç»ç½‘ç»œæ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
        num_epochs: æœ€å¤§è®­ç»ƒè½®æ•°
        patience: æ—©åœè€å¿ƒå€¼
        device: è®­ç»ƒè®¾å¤‡

    è¿”å›ï¼š
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        best_val_loss: æœ€ä½³éªŒè¯æŸå¤±
        history: è®­ç»ƒå†å²è®°å½•
    """
    model = model.to(device)
    best_val_loss = float("inf")
    patience_counter = 0
    best_model_state = None

    history = {
        "train_loss": [],
        "val_loss": [],
        "lr": [],
        "epoch_times": [],  # è®°å½•æ¯ä¸ªepochçš„è®­ç»ƒæ—¶é—´
    }

    print(f"å¼€å§‹è®­ç»ƒï¼Œè®¾å¤‡: {device}")
    start_time = time.time()

    # ä½¿ç”¨torch.ampè‡ªåŠ¨æ··åˆç²¾åº¦åŠ é€Ÿè®­ç»ƒï¼ˆé€‚ç”¨äºæ”¯æŒçš„è®¾å¤‡ï¼‰
    use_amp = device.type in ["cuda", "mps"]
    scaler = torch.cuda.amp.GradScaler() if use_amp and device.type == "cuda" else None

    for epoch in range(num_epochs):
        epoch_start = time.time()
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0

        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad(set_to_none=True)  # æ›´é«˜æ•ˆçš„æ¢¯åº¦æ¸…é›¶

            # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆå¯¹æ”¯æŒçš„è®¾å¤‡ï¼‰
            if use_amp and device.type == "cuda":
                with torch.cuda.amp.autocast():
                    outputs = model(X_batch).squeeze()
                    loss = criterion(outputs, y_batch)
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                outputs = model(X_batch).squeeze()
                loss = criterion(outputs, y_batch)
                loss.backward()
                optimizer.step()

            train_loss += loss.item()

        train_loss /= len(train_loader)

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                outputs = model(X_batch).squeeze()
                val_loss += criterion(outputs, y_batch).item()
        val_loss /= len(val_loader)

        # æ›´æ–°å­¦ä¹ ç‡
        if scheduler is not None:
            scheduler.step(val_loss)
            current_lr = optimizer.param_groups[0]["lr"]
        else:
            current_lr = optimizer.param_groups[0]["lr"]

        # è®°å½•å†å²
        epoch_time = time.time() - epoch_start
        history["train_loss"].append(train_loss)
        history["val_loss"].append(val_loss)
        history["lr"].append(current_lr)
        history["epoch_times"].append(epoch_time)

        # æ—©åœ
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = model.state_dict()
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch+1}")
                break

        if (epoch + 1) % 10 == 0 or epoch == 0:
            print(
                f"Epoch {epoch+1:3d} | Train Loss: {train_loss:.4e} | "
                f"Val Loss: {val_loss:.4e} | LR: {current_lr:.2e} | "
                f"Time: {epoch_time:.2f}s"
            )

    # åŠ è½½æœ€ä½³æ¨¡å‹
    total_time = time.time() - start_time
    print(f"æ€»è®­ç»ƒæ—¶é—´: {total_time:.2f}ç§’")
    model.load_state_dict(best_model_state)
    return model, best_val_loss, history


# %% [5] æ¨¡å‹è¯„ä¼°å’Œå¯è§†åŒ–
def evaluate_and_visualize(
    model: nn.Module,
    data_info: Dict[str, Any],
    model_name: str,
    device: str = device,
    result_dir: str = None,
    round_name: str = "",
) -> Dict[str, float]:
    """
    è¯„ä¼°æ¨¡å‹å¹¶å¯è§†åŒ–ç»“æœ

    å‚æ•°ï¼š
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        data_info: æ•°æ®ä¿¡æ¯å­—å…¸
        model_name: æ¨¡å‹åç§°
        device: è®¡ç®—è®¾å¤‡
        result_dir: ç»“æœä¿å­˜ç›®å½•
        round_name: è®­ç»ƒè½®æ¬¡åç§°

    è¿”å›:
        metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    model.eval()
    metrics = {}

    with torch.no_grad():
        # åˆ›å»ºæ›´å¤§èŒƒå›´çš„ç½‘æ ¼ç‚¹ï¼Œç¡®ä¿åŒ…å«æ‰©å±•åŒºåŸŸ
        # æ³¨æ„ï¼šx1èŒƒå›´ä¸º[-1.5, 1.5]ï¼Œx2èŒƒå›´ä¸º[-0.5, 2.0]
        x1 = np.linspace(-1.5, 1.5, 100)
        x2 = np.linspace(-0.5, 2.0, 100)
        X1, X2 = np.meshgrid(x1, x2)
        X_grid = np.stack([X1.flatten(), X2.flatten()], axis=1)

        # è®¡ç®—æ•´ä¸ªèŒƒå›´çš„çœŸå®åŠ¿èƒ½é¢ï¼ˆæ ¹æ®MÃ¼ller-BrownåŠ¿èƒ½å‡½æ•°ï¼‰
        U_true = muller_brown_potential(X1.flatten(), X2.flatten()).reshape(X1.shape)

        # æ¨¡å‹é¢„æµ‹
        X_grid_tensor = torch.tensor(X_grid, dtype=torch.float32).to(device)

        # æ‰¹å¤„ç†é¢„æµ‹ä»¥æé«˜æ€§èƒ½
        batch_size = 1000
        U_pred_list = []
        for i in range(0, len(X_grid), batch_size):
            batch = X_grid_tensor[i : i + batch_size]
            U_pred_batch = model(batch).cpu().numpy()
            U_pred_list.append(U_pred_batch)

        U_pred = np.concatenate(U_pred_list).reshape(X1.shape)

        # è®¡ç®—å·®å¼‚
        U_diff = U_pred - U_true

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        mae = np.mean(np.abs(U_diff))
        rmse = np.sqrt(np.mean(U_diff**2))
        max_error = np.max(np.abs(U_diff))
        # ç›¸å¯¹è¯¯å·®ç™¾åˆ†æ¯”
        rel_error = np.abs(U_diff) / (np.abs(U_true) + 1e-10) * 100
        mean_rel_error = np.mean(rel_error)

        metrics["MAE"] = mae
        metrics["RMSE"] = rmse
        metrics["æœ€å¤§è¯¯å·®"] = max_error
        metrics["å¹³å‡ç›¸å¯¹è¯¯å·®(%)"] = mean_rel_error

        # åˆ›å»ºå­å›¾
        fig = plt.figure(figsize=(15, 10))

        # 1. çœŸå®åŠ¿èƒ½é¢ (å¸¦è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ ‡è®°)
        ax1 = fig.add_subplot(231)
        im1 = ax1.imshow(
            U_true,
            extent=[x1[0], x1[-1], x2[0], x2[-1]],
            origin="lower",
            cmap="viridis",
            alpha=0.9,  # è°ƒé«˜é€æ˜åº¦ï¼Œä½¿ç‚¹é›†æ›´åŠ å¯è§
        )

        # æ·»åŠ è®­ç»ƒå’Œæµ‹è¯•é›†æ•°æ®ç‚¹ - æ”¹è¿›ç‚¹çš„æ˜¾ç¤ºæ•ˆæœ
        X_train = data_info["X_train"]
        X_val = data_info["X_val"]

        # è®­ç»ƒé›†ç‚¹ - ä½¿ç”¨æ›´å¥½çœ‹çš„é¢œè‰²å’Œå½¢çŠ¶ï¼Œå‡å°ç‚¹çš„å¤§å°
        ax1.scatter(
            X_train[:, 0],
            X_train[:, 1],
            s=1.5,  # å‡å°ç‚¹çš„å¤§å°
            c="cyan",  # æ›´æ”¹ä¸ºé’è‰²
            alpha=0.6,
            marker="o",
            label="è®­ç»ƒé›†",
        )

        # æµ‹è¯•é›†ç‚¹ - ä½¿ç”¨æ›´å¥½çœ‹çš„é¢œè‰²å’Œå½¢çŠ¶ï¼Œå‡å°ç‚¹çš„å¤§å°
        ax1.scatter(
            X_val[:, 0],
            X_val[:, 1],
            s=1.5,  # å‡å°ç‚¹çš„å¤§å°
            c="magenta",  # æ›´æ”¹ä¸ºå“çº¢è‰²
            alpha=0.6,
            marker="o",
            label="æµ‹è¯•é›†",
        )

        ax1.set_title("çœŸå®åŠ¿èƒ½é¢ (å¸¦æ•°æ®åˆ†å¸ƒ)")
        ax1.set_xlabel("xâ‚")
        ax1.set_ylabel("xâ‚‚")
        ax1.legend(loc="upper right", markerscale=3)  # å¢å¤§å›¾ä¾‹ä¸­çš„ç‚¹çš„å¤§å°
        fig.colorbar(im1, ax=ax1)

        # 2. é¢„æµ‹åŠ¿èƒ½é¢
        ax2 = fig.add_subplot(232)
        im2 = ax2.imshow(
            U_pred,
            extent=[x1[0], x1[-1], x2[0], x2[-1]],
            origin="lower",
            cmap="viridis",
        )
        ax2.set_title(f"{model_name}é¢„æµ‹")
        ax2.set_xlabel("xâ‚")
        ax2.set_ylabel("xâ‚‚")
        fig.colorbar(im2, ax=ax2)

        # 3. å·®å¼‚åˆ†å¸ƒ
        ax3 = fig.add_subplot(233)
        im3 = ax3.imshow(
            U_diff, extent=[x1[0], x1[-1], x2[0], x2[-1]], origin="lower", cmap="RdBu"
        )
        ax3.set_title("é¢„æµ‹å·®å¼‚ (é¢„æµ‹å€¼-çœŸå®å€¼)")
        ax3.set_xlabel("xâ‚")
        ax3.set_ylabel("xâ‚‚")
        fig.colorbar(im3, ax=ax3)

        # 4. ç›¸å¯¹è¯¯å·®ç™¾åˆ†æ¯”å›¾
        ax4 = fig.add_subplot(234)
        # é™åˆ¶ç›¸å¯¹è¯¯å·®çš„ä¸Šé™ï¼Œä½¿é¢œè‰²æ˜ å°„æ›´æœ‰åŒºåˆ†åº¦
        rel_error_clipped = np.clip(rel_error, 0, 50)
        im4 = ax4.imshow(
            rel_error_clipped,
            extent=[x1[0], x1[-1], x2[0], x2[-1]],
            origin="lower",
            cmap="Reds",
            vmin=0,
            vmax=50,
        )
        ax4.set_title("ç›¸å¯¹è¯¯å·®ç™¾åˆ†æ¯”")
        ax4.set_xlabel("xâ‚")
        ax4.set_ylabel("xâ‚‚")
        # åˆ›å»ºé¢œè‰²æ¡
        cbar = fig.colorbar(im4, ax=ax4)
        cbar.set_label("ç›¸å¯¹è¯¯å·® (%)")

        # 5. è¯¯å·®ç›´æ–¹å›¾
        ax5 = fig.add_subplot(235)
        hist_data = np.clip(
            rel_error.flatten(), 0, 50
        )  # é™åˆ¶æœ€å¤§å€¼ä¸º50%ä»¥ä¾¿æ›´å¥½åœ°å¯è§†åŒ–
        ax5.hist(hist_data, bins=40, alpha=0.7, color="red")
        ax5.set_title("ç›¸å¯¹è¯¯å·®ç›´æ–¹å›¾")
        ax5.set_xlabel("ç›¸å¯¹è¯¯å·® (%)")
        ax5.set_ylabel("é¢‘æ¬¡")

        # æ·»åŠ å¹³å‡ç›¸å¯¹è¯¯å·®å’Œæœ€å¤§è¯¯å·®æ ‡æ³¨
        ax5.axvline(
            mean_rel_error,
            color="black",
            linestyle="--",
            label=f"å¹³å‡: {mean_rel_error:.2f}%",
        )
        ax5.legend()

        # 6. æ•£ç‚¹å›¾: çœŸå®å€¼vsé¢„æµ‹å€¼ (å¸¦è¯¯å·®æŒ‡ç¤º)
        ax6 = fig.add_subplot(236)

        # ä½¿ç”¨åŸå§‹ç›¸å¯¹è¯¯å·®ç€è‰²
        scatter = ax6.scatter(
            U_true.flatten(),
            U_pred.flatten(),
            alpha=0.3,
            c=np.clip(rel_error.flatten(), 0, 50),
            cmap="plasma",
            vmin=0,
            vmax=50,
            label="é¢„æµ‹ç‚¹",
        )

        # æ·»åŠ x=yçš„ç†æƒ³é¢„æµ‹çº¿
        min_val = min(U_true.min(), U_pred.min())
        max_val = max(U_true.max(), U_pred.max())
        ax6.plot([min_val, max_val], [min_val, max_val], "r--", label="ç†æƒ³é¢„æµ‹çº¿")

        # æ·»åŠ Â±10%è¯¯å·®è¾¹ç•Œçº¿
        ax6.plot(
            [min_val, max_val],
            [min_val * 0.9, max_val * 0.9],
            "k:",
            alpha=0.5,
            label="-10%è¯¯å·®ç•Œ",
        )
        ax6.plot(
            [min_val, max_val],
            [min_val * 1.1, max_val * 1.1],
            "k:",
            alpha=0.5,
            label="+10%è¯¯å·®ç•Œ",
        )

        ax6.set_title("çœŸå®å€¼ vs é¢„æµ‹å€¼")
        ax6.set_xlabel("çœŸå®å€¼")
        ax6.set_ylabel("é¢„æµ‹å€¼")

        # æ·»åŠ é¢œè‰²æ¡ï¼Œä¿®å¤æ ‡ç­¾é—®é¢˜ï¼Œç¡®ä¿åŒ…å«ç™¾åˆ†å·
        cbar = fig.colorbar(scatter, ax=ax6)
        cbar.set_label("ç›¸å¯¹è¯¯å·® (%)")  # ç¡®ä¿åŒ…å«ç™¾åˆ†å·

        # æ·»åŠ å›¾ä¾‹
        ax6.legend(loc="upper left")

        plt.suptitle(f"{model_name}æ¨¡å‹è¯„ä¼°ç»“æœ")
        plt.tight_layout()

        # ä¿å­˜å›¾åƒåˆ°æŒ‡å®šç›®å½•
        if result_dir:
            model_type = model_name.split()[0]  # æå–æ¨¡å‹ç±»å‹ï¼ˆMLPã€StdNetã€ResNetï¼‰
            fig_name = f"{model_type}_{round_name}_è¯„ä¼°ç»“æœ.png"
            fig_path = os.path.join(result_dir, "figures", fig_name)
            plt.savefig(fig_path, dpi=300, bbox_inches="tight")
            print(f"ä¿å­˜è¯„ä¼°å›¾åƒ: {fig_path}")

        plt.show()

        # è¾“å‡ºè¯„ä¼°æŒ‡æ ‡
        print(f"\n{model_name}è¯„ä¼°æŒ‡æ ‡:")
        print(f"MAE: {mae:.4f}")
        print(f"RMSE: {rmse:.4f}")
        print(f"æœ€å¤§è¯¯å·®: {max_error:.4f}")
        print(f"å¹³å‡ç›¸å¯¹è¯¯å·®: {mean_rel_error:.2f}%")

        return metrics


# %% [6] è®¡ç®—æ¨¡å‹å‚æ•°é‡
def count_parameters(model: nn.Module) -> int:
    """è®¡ç®—æ¨¡å‹å¯è®­ç»ƒå‚æ•°æ•°é‡"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


# %% [7] ä¸»è®­ç»ƒæµç¨‹
def main():
    """ä¸»è®­ç»ƒæµç¨‹ï¼šæ‰§è¡Œä¸¤è½®è®­ç»ƒï¼Œåˆ†åˆ«ä½¿ç”¨ä¸åŒæ•°é‡çš„æ•°æ®ç‚¹"""

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)

    # åˆ›å»ºç»“æœä¿å­˜ç›®å½•
    result_dir = create_result_dir()

    # è®­ç»ƒå‚æ•°
    criterion = nn.MSELoss()
    scheduler_kwargs = {"mode": "min", "factor": 0.5, "patience": 5}

    # ä¿å­˜ä¸¤è½®è®­ç»ƒçš„æŒ‡æ ‡ç»“æœ
    all_metrics = {}
    all_history = {}

    # ============================== ç¬¬ä¸€è½®è®­ç»ƒï¼ˆåŸå§‹æ•°æ®ï¼‰==============================
    print("\n" + "=" * 70)
    print(f"ç¬¬ä¸€è½®è®­ç»ƒ: ä½¿ç”¨åŸå§‹æ•°æ®ç‚¹ (æ— å¢å¹¿)")
    print("=" * 70)

    # å‡†å¤‡æ•°æ® - ç¬¬ä¸€è½®ä½¿ç”¨åŸå§‹æ•°æ®ç‚¹ï¼Œä¸è¿›è¡Œå¢å¹¿
    train_loader_1, val_loader_1, data_info_1 = prepare_data(
        data_file="train_data.txt", augment=False
    )

    # è·å–æ•°æ®ç‚¹æ•°é‡
    n_points_1 = len(data_info_1["X_train"]) + len(data_info_1["X_val"])
    print(f"æ•°æ®é›†æ€»ç‚¹æ•°: {n_points_1}")

    # åˆ›å»ºæ¨¡å‹
    mlp_1 = MLP()
    stdnet_1 = StdNet(data_info_1["mean"], data_info_1["std"])
    resnet_1 = ResNet(data_info_1["mean"], data_info_1["std"])

    # è®¡ç®—å¹¶æ‰“å°æ¨¡å‹å‚æ•°é‡
    mlp_params = count_parameters(mlp_1)
    std_params = count_parameters(stdnet_1)
    res_params = count_parameters(resnet_1)
    print(f"MLPå‚æ•°é‡: {mlp_params:,}")
    print(f"StdNetå‚æ•°é‡: {std_params:,}")
    print(f"ResNetå‚æ•°é‡: {res_params:,}")

    # è®­ç»ƒç¬¬ä¸€è½®MLP
    print("\nè®­ç»ƒMLP (ç¬¬ä¸€è½®)...")
    optimizer_mlp_1 = torch.optim.Adam(mlp_1.parameters(), lr=0.001)
    scheduler_mlp_1 = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer_mlp_1, **scheduler_kwargs
    )
    mlp_1, best_val_mlp_1, history_mlp_1 = train_model(
        mlp_1,
        train_loader_1,
        val_loader_1,
        criterion,
        optimizer_mlp_1,
        scheduler_mlp_1,
        patience=50,
        device=device,
    )

    # è®­ç»ƒç¬¬ä¸€è½®StdNet
    print("\nè®­ç»ƒStdNet (ç¬¬ä¸€è½®)...")
    optimizer_std_1 = torch.optim.Adam(stdnet_1.parameters(), lr=0.001)
    scheduler_std_1 = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer_std_1, **scheduler_kwargs
    )
    stdnet_1, best_val_std_1, history_std_1 = train_model(
        stdnet_1,
        train_loader_1,
        val_loader_1,
        criterion,
        optimizer_std_1,
        scheduler_std_1,
        patience=150,
        device=device,
    )

    # è®­ç»ƒç¬¬ä¸€è½®ResNet
    print("\nè®­ç»ƒResNet (ç¬¬ä¸€è½®)...")
    optimizer_res_1 = torch.optim.Adam(resnet_1.parameters(), lr=0.001)
    scheduler_res_1 = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer_res_1, **scheduler_kwargs
    )
    resnet_1, best_val_res_1, history_res_1 = train_model(
        resnet_1,
        train_loader_1,
        val_loader_1,
        criterion,
        optimizer_res_1,
        scheduler_res_1,
        patience=150,
        device=device,
    )

    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp_1 = get_timestamp()

    # ä¿å­˜ç¬¬ä¸€è½®è®­ç»ƒçš„æ¨¡å‹ï¼ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
    model_dir = os.path.join(result_dir, "models")
    torch.save(
        mlp_1.state_dict(),
        os.path.join(model_dir, f"mlp_data{n_points_1}_{timestamp_1}.pth"),
    )
    torch.save(
        stdnet_1.state_dict(),
        os.path.join(model_dir, f"stdnet_data{n_points_1}_{timestamp_1}.pth"),
    )
    torch.save(
        resnet_1.state_dict(),
        os.path.join(model_dir, f"resnet_data{n_points_1}_{timestamp_1}.pth"),
    )

    # è¯„ä¼°ç¬¬ä¸€è½®æ¨¡å‹
    print("\nè¯„ä¼°MLP (ç¬¬ä¸€è½®)...")
    metrics_mlp_1 = evaluate_and_visualize(
        mlp_1,
        data_info_1,
        "MLP (åŸå§‹æ•°æ®)",
        device=device,
        result_dir=result_dir,
        round_name="round1",
    )

    print("\nè¯„ä¼°StdNet (ç¬¬ä¸€è½®)...")
    metrics_std_1 = evaluate_and_visualize(
        stdnet_1,
        data_info_1,
        "StdNet (åŸå§‹æ•°æ®)",
        device=device,
        result_dir=result_dir,
        round_name="round1",
    )

    print("\nè¯„ä¼°ResNet (ç¬¬ä¸€è½®)...")
    metrics_res_1 = evaluate_and_visualize(
        resnet_1,
        data_info_1,
        "ResNet (åŸå§‹æ•°æ®)",
        device=device,
        result_dir=result_dir,
        round_name="round1",
    )

    # ä¿å­˜ç¬¬ä¸€è½®è®­ç»ƒçš„å†å²å’ŒæŒ‡æ ‡
    all_history["round1"] = {
        "MLP": history_mlp_1,
        "StdNet": history_std_1,
        "ResNet": history_res_1,
    }

    all_metrics["round1"] = {
        "MLP": metrics_mlp_1,
        "StdNet": metrics_std_1,
        "ResNet": metrics_res_1,
    }

    # ============================== ç¬¬äºŒè½®è®­ç»ƒï¼ˆå¢å¹¿æ•°æ®ï¼‰==============================
    print("\n" + "=" * 70)
    print(f"ç¬¬äºŒè½®è®­ç»ƒ: ä½¿ç”¨å¢å¹¿æ•°æ®ç‚¹ (åŸå§‹æ•°æ® + 2000ä¸ªå¢å¹¿ç‚¹)")
    print("=" * 70)

    # å‡†å¤‡æ•°æ® - ç¬¬äºŒè½®ä½¿ç”¨å¢å¹¿æ•°æ®
    train_loader_2, val_loader_2, data_info_2 = prepare_data(
        data_file="train_data.txt", augment=True, n_augment=2000
    )

    # è·å–æ•°æ®ç‚¹æ•°é‡
    n_points_2 = len(data_info_2["X_train"]) + len(data_info_2["X_val"])
    print(f"å¢å¹¿åæ•°æ®é›†æ€»ç‚¹æ•°: {n_points_2}")

    # åˆ›å»ºæ¨¡å‹
    mlp_2 = MLP()
    stdnet_2 = StdNet(data_info_2["mean"], data_info_2["std"])
    resnet_2 = ResNet(data_info_2["mean"], data_info_2["std"])

    # è®­ç»ƒç¬¬äºŒè½®MLP
    print("\nè®­ç»ƒMLP (ç¬¬äºŒè½®)...")
    optimizer_mlp_2 = torch.optim.Adam(mlp_2.parameters(), lr=0.001)
    scheduler_mlp_2 = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer_mlp_2, **scheduler_kwargs
    )
    mlp_2, best_val_mlp_2, history_mlp_2 = train_model(
        mlp_2,
        train_loader_2,
        val_loader_2,
        criterion,
        optimizer_mlp_2,
        scheduler_mlp_2,
        patience=50,
        device=device,
    )

    # è®­ç»ƒç¬¬äºŒè½®StdNet
    print("\nè®­ç»ƒStdNet (ç¬¬äºŒè½®)...")
    optimizer_std_2 = torch.optim.Adam(stdnet_2.parameters(), lr=0.001)
    scheduler_std_2 = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer_std_2, **scheduler_kwargs
    )
    stdnet_2, best_val_std_2, history_std_2 = train_model(
        stdnet_2,
        train_loader_2,
        val_loader_2,
        criterion,
        optimizer_std_2,
        scheduler_std_2,
        patience=150,
        device=device,
    )

    # è®­ç»ƒç¬¬äºŒè½®ResNet
    print("\nè®­ç»ƒResNet (ç¬¬äºŒè½®)...")
    optimizer_res_2 = torch.optim.Adam(resnet_2.parameters(), lr=0.001)
    scheduler_res_2 = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer_res_2, **scheduler_kwargs
    )
    resnet_2, best_val_res_2, history_res_2 = train_model(
        resnet_2,
        train_loader_2,
        val_loader_2,
        criterion,
        optimizer_res_2,
        scheduler_res_2,
        patience=150,
        device=device,
    )

    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp_2 = get_timestamp()

    # ä¿å­˜ç¬¬äºŒè½®è®­ç»ƒçš„æ¨¡å‹ï¼ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
    torch.save(
        mlp_2.state_dict(),
        os.path.join(model_dir, f"mlp_data{n_points_2}_{timestamp_2}.pth"),
    )
    torch.save(
        stdnet_2.state_dict(),
        os.path.join(model_dir, f"stdnet_data{n_points_2}_{timestamp_2}.pth"),
    )
    torch.save(
        resnet_2.state_dict(),
        os.path.join(model_dir, f"resnet_data{n_points_2}_{timestamp_2}.pth"),
    )

    # è¯„ä¼°ç¬¬äºŒè½®æ¨¡å‹
    print("\nè¯„ä¼°MLP (ç¬¬äºŒè½®)...")
    metrics_mlp_2 = evaluate_and_visualize(
        mlp_2,
        data_info_2,
        "MLP (å¢å¹¿æ•°æ®)",
        device=device,
        result_dir=result_dir,
        round_name="round2",
    )

    print("\nè¯„ä¼°StdNet (ç¬¬äºŒè½®)...")
    metrics_std_2 = evaluate_and_visualize(
        stdnet_2,
        data_info_2,
        "StdNet (å¢å¹¿æ•°æ®)",
        device=device,
        result_dir=result_dir,
        round_name="round2",
    )

    print("\nè¯„ä¼°ResNet (ç¬¬äºŒè½®)...")
    metrics_res_2 = evaluate_and_visualize(
        resnet_2,
        data_info_2,
        "ResNet (å¢å¹¿æ•°æ®)",
        device=device,
        result_dir=result_dir,
        round_name="round2",
    )

    # ä¿å­˜ç¬¬äºŒè½®è®­ç»ƒçš„å†å²å’ŒæŒ‡æ ‡
    all_history["round2"] = {
        "MLP": history_mlp_2,
        "StdNet": history_std_2,
        "ResNet": history_res_2,
    }

    all_metrics["round2"] = {
        "MLP": metrics_mlp_2,
        "StdNet": metrics_std_2,
        "ResNet": metrics_res_2,
    }

    # ============================== å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹å’Œå¯¹æ¯”ç»“æœ ==============================

    # åˆ›å»ºå¤šä¸ªåˆ†é¡µçš„å›¾è¡¨ï¼Œä»¥ä¾¿æ›´æ¸…æ™°åœ°å±•ç¤ºç»“æœ
    # ç¬¬ä¸€é¡µï¼šè®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±å¯¹æ¯”
    plt.figure(figsize=(15, 10))

    # ç¬¬ä¸€è½®è®­ç»ƒçš„æŸå¤±æ›²çº¿
    plt.subplot(221)
    plt.plot(all_history["round1"]["MLP"]["train_loss"], label="MLP è®­ç»ƒ")
    plt.plot(all_history["round1"]["MLP"]["val_loss"], linestyle="--", label="MLP éªŒè¯")
    plt.plot(all_history["round1"]["StdNet"]["train_loss"], label="StdNet è®­ç»ƒ")
    plt.plot(
        all_history["round1"]["StdNet"]["val_loss"], linestyle="--", label="StdNet éªŒè¯"
    )
    plt.plot(all_history["round1"]["ResNet"]["train_loss"], label="ResNet è®­ç»ƒ")
    plt.plot(
        all_history["round1"]["ResNet"]["val_loss"], linestyle="--", label="ResNet éªŒè¯"
    )
    plt.yscale("log")  # ä½¿ç”¨å¯¹æ•°å°ºåº¦æ›´å¥½åœ°æ˜¾ç¤ºæŸå¤±æ›²çº¿
    plt.xlabel("è¿­ä»£è½®æ•°")
    plt.ylabel("æŸå¤±å€¼ (å¯¹æ•°å°ºåº¦)")
    plt.title("ç¬¬ä¸€è½®è®­ç»ƒ: åŸå§‹æ•°æ®æŸå¤±æ›²çº¿")
    plt.legend()
    plt.grid(True, alpha=0.3)

    # ç¬¬äºŒè½®è®­ç»ƒçš„æŸå¤±æ›²çº¿
    plt.subplot(222)
    plt.plot(all_history["round2"]["MLP"]["train_loss"], label="MLP è®­ç»ƒ")
    plt.plot(all_history["round2"]["MLP"]["val_loss"], linestyle="--", label="MLP éªŒè¯")
    plt.plot(all_history["round2"]["StdNet"]["train_loss"], label="StdNet è®­ç»ƒ")
    plt.plot(
        all_history["round2"]["StdNet"]["val_loss"], linestyle="--", label="StdNet éªŒè¯"
    )
    plt.plot(all_history["round2"]["ResNet"]["train_loss"], label="ResNet è®­ç»ƒ")
    plt.plot(
        all_history["round2"]["ResNet"]["val_loss"], linestyle="--", label="ResNet éªŒè¯"
    )
    plt.yscale("log")  # ä½¿ç”¨å¯¹æ•°å°ºåº¦æ›´å¥½åœ°æ˜¾ç¤ºæŸå¤±æ›²çº¿
    plt.xlabel("è¿­ä»£è½®æ•°")
    plt.ylabel("æŸå¤±å€¼ (å¯¹æ•°å°ºåº¦)")
    plt.title("ç¬¬äºŒè½®è®­ç»ƒ: å¢å¹¿æ•°æ®æŸå¤±æ›²çº¿")
    plt.legend()
    plt.grid(True, alpha=0.3)

    # å­¦ä¹ ç‡å˜åŒ–å¯¹æ¯” (ç¬¬ä¸€è½®)
    plt.subplot(223)
    plt.plot(all_history["round1"]["MLP"]["lr"], label="MLP")
    plt.plot(all_history["round1"]["StdNet"]["lr"], label="StdNet")
    plt.plot(all_history["round1"]["ResNet"]["lr"], label="ResNet")
    plt.yscale("log")  # å¯¹æ•°å°ºåº¦
    plt.xlabel("è¿­ä»£è½®æ•°")
    plt.ylabel("å­¦ä¹ ç‡ (å¯¹æ•°å°ºåº¦)")
    plt.title("ç¬¬ä¸€è½®è®­ç»ƒ: å­¦ä¹ ç‡å˜åŒ–")
    plt.legend()
    plt.grid(True, alpha=0.3)

    # å­¦ä¹ ç‡å˜åŒ–å¯¹æ¯” (ç¬¬äºŒè½®)
    plt.subplot(224)
    plt.plot(all_history["round2"]["MLP"]["lr"], label="MLP")
    plt.plot(all_history["round2"]["StdNet"]["lr"], label="StdNet")
    plt.plot(all_history["round2"]["ResNet"]["lr"], label="ResNet")
    plt.yscale("log")  # å¯¹æ•°å°ºåº¦
    plt.xlabel("è¿­ä»£è½®æ•°")
    plt.ylabel("å­¦ä¹ ç‡ (å¯¹æ•°å°ºåº¦)")
    plt.title("ç¬¬äºŒè½®è®­ç»ƒ: å­¦ä¹ ç‡å˜åŒ–")
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.suptitle("è®­ç»ƒè¿‡ç¨‹å¯¹æ¯”: åŸå§‹æ•°æ® vs å¢å¹¿æ•°æ®", y=1.02, fontsize=16)

    # ä¿å­˜è®­ç»ƒè¿‡ç¨‹å¯¹æ¯”å›¾
    plt.savefig(
        os.path.join(result_dir, "figures", "è®­ç»ƒè¿‡ç¨‹å¯¹æ¯”.png"),
        dpi=300,
        bbox_inches="tight",
    )
    plt.show()

    # ç¬¬äºŒé¡µï¼šè®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
    plt.figure(figsize=(15, 12))

    # è®­ç»ƒæ•ˆç‡å¯¹æ¯” - ä½¿ç”¨å¯¹æ•°å°ºåº¦æˆ–è°ƒæ•´yè½´èŒƒå›´
    plt.subplot(221)
    models = ["MLP", "StdNet", "ResNet"]
    # è®¡ç®—å„ä¸ªæ¨¡å‹åœ¨ä¸¤è½®è®­ç»ƒä¸­çš„å¹³å‡æ¯è½®è®­ç»ƒæ—¶é—´
    avg_times_1 = [
        np.mean(all_history["round1"]["MLP"]["epoch_times"]),
        np.mean(all_history["round1"]["StdNet"]["epoch_times"]),
        np.mean(all_history["round1"]["ResNet"]["epoch_times"]),
    ]
    avg_times_2 = [
        np.mean(all_history["round2"]["MLP"]["epoch_times"]),
        np.mean(all_history["round2"]["StdNet"]["epoch_times"]),
        np.mean(all_history["round2"]["ResNet"]["epoch_times"]),
    ]

    x = np.arange(len(models))
    width = 0.35

    # è°ƒæ•´yè½´æœ€å¤§å€¼ä»¥ç¡®ä¿æ‰€æœ‰æŸ±å­å¯è§
    max_time = max(max(avg_times_1), max(avg_times_2))

    bars1 = plt.bar(x - width / 2, avg_times_1, width, label="åŸå§‹æ•°æ®", alpha=0.7)
    bars2 = plt.bar(x + width / 2, avg_times_2, width, label="å¢å¹¿æ•°æ®", alpha=0.7)

    plt.ylabel("å¹³å‡æ¯è½®è®­ç»ƒæ—¶é—´ (ç§’)")
    plt.title("è®­ç»ƒæ•ˆç‡å¯¹æ¯”")
    plt.xticks(x, models)
    plt.ylim(0, max_time * 1.1)  # è®¾ç½®yè½´ä¸Šé™ä¸ºæœ€å¤§å€¼çš„1.1å€
    plt.legend()

    # æ·»åŠ å…·ä½“æ•°å€¼æ ‡ç­¾
    for i, bars in enumerate([bars1, bars2]):
        for bar in bars:
            height = bar.get_height()
            plt.text(
                bar.get_x() + bar.get_width() / 2.0,
                height + max_time * 0.01,
                f"{height:.4f}s",
                ha="center",
                va="bottom",
                fontsize=8,
            )
    plt.grid(True, alpha=0.3)

    # ä¸¤è½®è®­ç»ƒçš„MAEå¯¹æ¯”
    plt.subplot(222)
    metrics_names = ["MAE", "RMSE", "æœ€å¤§è¯¯å·®", "å¹³å‡ç›¸å¯¹è¯¯å·®(%)"]
    mae_1 = [
        all_metrics["round1"]["MLP"]["MAE"],
        all_metrics["round1"]["StdNet"]["MAE"],
        all_metrics["round1"]["ResNet"]["MAE"],
    ]
    mae_2 = [
        all_metrics["round2"]["MLP"]["MAE"],
        all_metrics["round2"]["StdNet"]["MAE"],
        all_metrics["round2"]["ResNet"]["MAE"],
    ]

    bars1 = plt.bar(x - width / 2, mae_1, width, label="åŸå§‹æ•°æ®", alpha=0.7)
    bars2 = plt.bar(x + width / 2, mae_2, width, label="å¢å¹¿æ•°æ®", alpha=0.7)

    plt.ylabel("MAE (è¶Šä½è¶Šå¥½)")
    plt.title("MAEå¯¹æ¯”: åŸå§‹æ•°æ® vs å¢å¹¿æ•°æ®")
    plt.xticks(x, models)
    plt.legend()

    # æ·»åŠ å…·ä½“æ•°å€¼æ ‡ç­¾
    for bar in bars1:
        height = bar.get_height()
        plt.text(
            bar.get_x() + bar.get_width() / 2.0,
            height + 0.0001,
            f"{height:.4f}",
            ha="center",
            va="bottom",
            fontsize=8,
        )
    for bar in bars2:
        height = bar.get_height()
        plt.text(
            bar.get_x() + bar.get_width() / 2.0,
            height + 0.0001,
            f"{height:.4f}",
            ha="center",
            va="bottom",
            fontsize=8,
        )
    plt.grid(True, alpha=0.3)

    # å¹³å‡ç›¸å¯¹è¯¯å·®å¯¹æ¯”
    plt.subplot(223)
    rel_err_1 = [
        all_metrics["round1"]["MLP"]["å¹³å‡ç›¸å¯¹è¯¯å·®(%)"],
        all_metrics["round1"]["StdNet"]["å¹³å‡ç›¸å¯¹è¯¯å·®(%)"],
        all_metrics["round1"]["ResNet"]["å¹³å‡ç›¸å¯¹è¯¯å·®(%)"],
    ]
    rel_err_2 = [
        all_metrics["round2"]["MLP"]["å¹³å‡ç›¸å¯¹è¯¯å·®(%)"],
        all_metrics["round2"]["StdNet"]["å¹³å‡ç›¸å¯¹è¯¯å·®(%)"],
        all_metrics["round2"]["ResNet"]["å¹³å‡ç›¸å¯¹è¯¯å·®(%)"],
    ]

    bars1 = plt.bar(x - width / 2, rel_err_1, width, label="åŸå§‹æ•°æ®", alpha=0.7)
    bars2 = plt.bar(x + width / 2, rel_err_2, width, label="å¢å¹¿æ•°æ®", alpha=0.7)

    plt.ylabel("å¹³å‡ç›¸å¯¹è¯¯å·® (%)")
    plt.title("å¹³å‡ç›¸å¯¹è¯¯å·®å¯¹æ¯”: åŸå§‹æ•°æ® vs å¢å¹¿æ•°æ®")
    plt.xticks(x, models)
    plt.legend()

    # æ·»åŠ å…·ä½“æ•°å€¼æ ‡ç­¾
    for bar in bars1:
        height = bar.get_height()
        plt.text(
            bar.get_x() + bar.get_width() / 2.0,
            height + 0.2,
            f"{height:.2f}%",
            ha="center",
            va="bottom",
            fontsize=8,
        )
    for bar in bars2:
        height = bar.get_height()
        plt.text(
            bar.get_x() + bar.get_width() / 2.0,
            height + 0.2,
            f"{height:.2f}%",
            ha="center",
            va="bottom",
            fontsize=8,
        )
    plt.grid(True, alpha=0.3)

    # æœ€å¤§è¯¯å·®å¯¹æ¯”
    plt.subplot(224)
    max_err_1 = [
        all_metrics["round1"]["MLP"]["æœ€å¤§è¯¯å·®"],
        all_metrics["round1"]["StdNet"]["æœ€å¤§è¯¯å·®"],
        all_metrics["round1"]["ResNet"]["æœ€å¤§è¯¯å·®"],
    ]
    max_err_2 = [
        all_metrics["round2"]["MLP"]["æœ€å¤§è¯¯å·®"],
        all_metrics["round2"]["StdNet"]["æœ€å¤§è¯¯å·®"],
        all_metrics["round2"]["ResNet"]["æœ€å¤§è¯¯å·®"],
    ]

    bars1 = plt.bar(x - width / 2, max_err_1, width, label="åŸå§‹æ•°æ®", alpha=0.7)
    bars2 = plt.bar(x + width / 2, max_err_2, width, label="å¢å¹¿æ•°æ®", alpha=0.7)

    plt.ylabel("æœ€å¤§è¯¯å·® (è¶Šä½è¶Šå¥½)")
    plt.title("æœ€å¤§è¯¯å·®å¯¹æ¯”: åŸå§‹æ•°æ® vs å¢å¹¿æ•°æ®")
    plt.xticks(x, models)
    plt.legend()

    # æ·»åŠ å…·ä½“æ•°å€¼æ ‡ç­¾
    for bar in bars1:
        height = bar.get_height()
        plt.text(
            bar.get_x() + bar.get_width() / 2.0,
            height + 0.01,
            f"{height:.4f}",
            ha="center",
            va="bottom",
            fontsize=8,
        )
    for bar in bars2:
        height = bar.get_height()
        plt.text(
            bar.get_x() + bar.get_width() / 2.0,
            height + 0.01,
            f"{height:.4f}",
            ha="center",
            va="bottom",
            fontsize=8,
        )
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.suptitle("æ¨¡å‹æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”: åŸå§‹æ•°æ® vs å¢å¹¿æ•°æ®", y=1.02, fontsize=16)

    # ä¿å­˜æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”å›¾
    plt.savefig(
        os.path.join(result_dir, "figures", "æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”.png"),
        dpi=300,
        bbox_inches="tight",
    )
    plt.show()

    # ç¬¬ä¸‰é¡µï¼šæ¶æ„å¯¹æ¯”å›¾ï¼ˆä¿æŒä¸å˜ï¼Œä½†æ·»åŠ æ›´æ¸…æ™°çš„æ ‡é¢˜ï¼‰
    plt.figure(figsize=(18, 10))

    # MLPæ¶æ„å›¾
    plt.subplot(131)
    mlp_layers = [
        "è¾“å…¥\n(2)",
        f"FC1\n(2Ã—128={2*128})",
        "ReLU",
        f"FC2\n(128Ã—128={128*128})",
        "ReLU",
        f"FC3\n(128Ã—64={128*64})",
        "ReLU",
        f"FC4\n(64Ã—1={64*1})",
        "è¾“å‡º\n(1)",
    ]
    positions = range(len(mlp_layers))
    plt.barh(positions, [0.7] * len(positions), color="lightgreen", alpha=0.7)

    for i, (pos, layer) in enumerate(zip(positions, mlp_layers)):
        plt.text(
            0.35, pos, layer, ha="center", va="center", fontsize=11, fontweight="bold"
        )

    plt.yticks([])
    plt.xticks([])
    plt.xlim([0, 0.7])
    plt.title(f"MLPæ¶æ„\næ€»å‚æ•°é‡: {mlp_params:,}", fontsize=14)
    plt.gca().invert_yaxis()

    # StdNetæ¶æ„å›¾
    plt.subplot(132)
    stdnet_layers = [
        "è¾“å…¥\n(2)",
        "æ ‡å‡†åŒ–å±‚\n(å‡å€¼æ–¹å·®)",
        f"FC1\n(2Ã—256={2*256})",
        "ReLU",
        "Dropout\n(p=0.1)",
        f"FC2\n(256Ã—128={256*128})",
        "ReLU",
        f"FC3\n(128Ã—1={128*1})",
        "è¾“å‡º\n(1)",
    ]
    positions = range(len(stdnet_layers))
    plt.barh(positions, [0.7] * len(positions), color="lightblue", alpha=0.7)

    for i, (pos, layer) in enumerate(zip(positions, stdnet_layers)):
        plt.text(
            0.35, pos, layer, ha="center", va="center", fontsize=11, fontweight="bold"
        )

    plt.yticks([])
    plt.xticks([])
    plt.xlim([0, 0.7])
    plt.title(f"StdNetæ¶æ„\næ€»å‚æ•°é‡: {std_params:,}", fontsize=14)
    plt.gca().invert_yaxis()

    # ResNetæ¶æ„å›¾
    plt.subplot(133)
    resnet_layers = [
        "è¾“å…¥\n(2)",
        "æ ‡å‡†åŒ–å±‚\n(å‡å€¼æ–¹å·®)",
        f"è¾“å…¥å±‚\n(2Ã—128={2*128})",
        "ReLU",
        "æ®‹å·®å—1",
        f"  FC1\n(128Ã—128={128*128})",
        "  ReLU",
        f"  FC2\n(128Ã—128={128*128})",
        "  Dropout(p=0.1)",
        "  + æ®‹å·®è¿æ¥",
        "æ®‹å·®å—2",
        f"  FC1\n(128Ã—128={128*128})",
        "  ReLU",
        f"  FC2\n(128Ã—128={128*128})",
        "  Dropout(p=0.1)",
        "  + æ®‹å·®è¿æ¥",
        f"è¾“å‡ºå±‚\n(128Ã—1={128*1})",
        "è¾“å‡º\n(1)",
    ]
    positions = range(len(resnet_layers))
    plt.barh(positions, [0.7] * len(positions), color="salmon", alpha=0.7)

    for i, (pos, layer) in enumerate(zip(positions, resnet_layers)):
        plt.text(
            0.35, pos, layer, ha="center", va="center", fontsize=10, fontweight="bold"
        )

    plt.yticks([])
    plt.xticks([])
    plt.xlim([0, 0.7])
    plt.title(f"ResNetæ¶æ„\næ€»å‚æ•°é‡: {res_params:,}", fontsize=14)
    plt.gca().invert_yaxis()

    plt.tight_layout()
    plt.suptitle("æ¨¡å‹æ¶æ„å¯¹æ¯”", y=0.98, fontsize=16, fontweight="bold")
    plt.subplots_adjust(top=0.9)

    # ä¿å­˜æ¨¡å‹æ¶æ„å¯¹æ¯”å›¾
    plt.savefig(
        os.path.join(result_dir, "figures", "æ¨¡å‹æ¶æ„å¯¹æ¯”.png"),
        dpi=300,
        bbox_inches="tight",
    )
    plt.show()

    # ä¿å­˜è¯¦ç»†æ¯”è¾ƒæŠ¥å‘Šåˆ°æ–‡æœ¬æ–‡ä»¶
    report_file = os.path.join(result_dir, "æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š.txt")
    with open(report_file, "w", encoding="utf-8") as f:
        f.write("=" * 70 + "\n")
        f.write("æ¨¡å‹å¯¹æ¯”è¯¦ç»†æŠ¥å‘Šï¼šåŸå§‹æ•°æ® vs å¢å¹¿æ•°æ®".center(60) + "\n")
        f.write("=" * 70 + "\n\n")

        # åˆ›å»ºè¡¨æ ¼æ ¼å¼
        format_row = "{:20} | {:^15} | {:^15} | {:^15}\n"
        f.write(format_row.format("æŒ‡æ ‡", "MLP", "StdNet", "ResNet"))
        f.write("-" * 68 + "\n")

        # æ¨¡å‹åŸºæœ¬ä¿¡æ¯
        f.write(
            format_row.format(
                "æ¨¡å‹å‚æ•°é‡", f"{mlp_params:,}", f"{std_params:,}", f"{res_params:,}"
            )
        )
        f.write(format_row.format("æ ‡å‡†åŒ–å±‚", "å¦", "æ˜¯", "æ˜¯"))
        f.write(format_row.format("Dropoutæ­£åˆ™åŒ–", "å¦", "æ˜¯", "æ˜¯"))
        f.write(format_row.format("æ®‹å·®è¿æ¥", "å¦", "å¦", "æ˜¯"))

        # æŒ‰ç…§ä¸¤è½®è®­ç»ƒåˆ†åˆ«è¾“å‡ºç»“æœ
        f.write("\n[ç¬¬ä¸€è½®è®­ç»ƒ - åŸå§‹æ•°æ®]\n")

        # è®­ç»ƒä¿¡æ¯
        f.write(
            format_row.format(
                "è®­ç»ƒè½®æ•°",
                len(all_history["round1"]["MLP"]["train_loss"]),
                len(all_history["round1"]["StdNet"]["train_loss"]),
                len(all_history["round1"]["ResNet"]["train_loss"]),
            )
        )
        total_time_mlp_1 = sum(all_history["round1"]["MLP"]["epoch_times"])
        total_time_std_1 = sum(all_history["round1"]["StdNet"]["epoch_times"])
        total_time_res_1 = sum(all_history["round1"]["ResNet"]["epoch_times"])
        f.write(
            format_row.format(
                "æ€»è®­ç»ƒæ—¶é—´(ç§’)",
                f"{total_time_mlp_1:.2f}",
                f"{total_time_std_1:.2f}",
                f"{total_time_res_1:.2f}",
            )
        )
        f.write(
            format_row.format(
                "å¹³å‡æ¯è½®æ—¶é—´(ç§’)",
                f"{np.mean(all_history['round1']['MLP']['epoch_times']):.4f}",
                f"{np.mean(all_history['round1']['StdNet']['epoch_times']):.4f}",
                f"{np.mean(all_history['round1']['ResNet']['epoch_times']):.4f}",
            )
        )
        f.write(
            format_row.format(
                "æœ€ç»ˆå­¦ä¹ ç‡",
                f"{all_history['round1']['MLP']['lr'][-1]:.2e}",
                f"{all_history['round1']['StdNet']['lr'][-1]:.2e}",
                f"{all_history['round1']['ResNet']['lr'][-1]:.2e}",
            )
        )

        # æ€§èƒ½æŒ‡æ ‡
        for metric in metrics_names:
            f.write(
                format_row.format(
                    metric,
                    f"{all_metrics['round1']['MLP'][metric]:.4f}",
                    f"{all_metrics['round1']['StdNet'][metric]:.4f}",
                    f"{all_metrics['round1']['ResNet'][metric]:.4f}",
                )
            )

        # ç¬¬äºŒè½®ç»“æœ
        f.write("\n[ç¬¬äºŒè½®è®­ç»ƒ - å¢å¹¿æ•°æ®]\n")

        # è®­ç»ƒä¿¡æ¯
        f.write(
            format_row.format(
                "è®­ç»ƒè½®æ•°",
                len(all_history["round2"]["MLP"]["train_loss"]),
                len(all_history["round2"]["StdNet"]["train_loss"]),
                len(all_history["round2"]["ResNet"]["train_loss"]),
            )
        )
        total_time_mlp_2 = sum(all_history["round2"]["MLP"]["epoch_times"])
        total_time_std_2 = sum(all_history["round2"]["StdNet"]["epoch_times"])
        total_time_res_2 = sum(all_history["round2"]["ResNet"]["epoch_times"])
        f.write(
            format_row.format(
                "æ€»è®­ç»ƒæ—¶é—´(ç§’)",
                f"{total_time_mlp_2:.2f}",
                f"{total_time_std_2:.2f}",
                f"{total_time_res_2:.2f}",
            )
        )
        f.write(
            format_row.format(
                "å¹³å‡æ¯è½®æ—¶é—´(ç§’)",
                f"{np.mean(all_history['round2']['MLP']['epoch_times']):.4f}",
                f"{np.mean(all_history['round2']['StdNet']['epoch_times']):.4f}",
                f"{np.mean(all_history['round2']['ResNet']['epoch_times']):.4f}",
            )
        )
        f.write(
            format_row.format(
                "æœ€ç»ˆå­¦ä¹ ç‡",
                f"{all_history['round2']['MLP']['lr'][-1]:.2e}",
                f"{all_history['round2']['StdNet']['lr'][-1]:.2e}",
                f"{all_history['round2']['ResNet']['lr'][-1]:.2e}",
            )
        )

        # æ€§èƒ½æŒ‡æ ‡
        for metric in metrics_names:
            f.write(
                format_row.format(
                    metric,
                    f"{all_metrics['round2']['MLP'][metric]:.4f}",
                    f"{all_metrics['round2']['StdNet'][metric]:.4f}",
                    f"{all_metrics['round2']['ResNet'][metric]:.4f}",
                )
            )

        # æ”¹è¿›ç™¾åˆ†æ¯”åˆ†æ
        f.write("\n[æ•°æ®å¢å¹¿å¸¦æ¥çš„æ€§èƒ½æ”¹è¿›]\n")
        for model in ["MLP", "StdNet", "ResNet"]:
            mae_improve = (
                (
                    all_metrics["round1"][model]["MAE"]
                    - all_metrics["round2"][model]["MAE"]
                )
                / all_metrics["round1"][model]["MAE"]
                * 100
            )
            rel_err_improve = (
                (
                    all_metrics["round1"][model]["å¹³å‡ç›¸å¯¹è¯¯å·®(%)"]
                    - all_metrics["round2"][model]["å¹³å‡ç›¸å¯¹è¯¯å·®(%)"]
                )
                / all_metrics["round1"][model]["å¹³å‡ç›¸å¯¹è¯¯å·®(%)"]
                * 100
            )
            f.write(
                f"{model}: MAEæ”¹è¿› {mae_improve:.2f}%, ç›¸å¯¹è¯¯å·®æ”¹è¿› {rel_err_improve:.2f}%\n"
            )

    # å°†è®­ç»ƒå†å²æ•°æ®ä¿å­˜ä¸ºJSON
    history_file = os.path.join(result_dir, "è®­ç»ƒå†å²æ•°æ®.json")

    # è½¬æ¢NumPyæ•°ç»„å’Œå…¶ä»–ä¸å¯JSONåºåˆ—åŒ–çš„å¯¹è±¡
    def convert_for_json(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.float32) or isinstance(obj, np.float64):
            return float(obj)
        elif isinstance(obj, np.int32) or isinstance(obj, np.int64):
            return int(obj)
        else:
            return obj

    # å‡†å¤‡è¦ä¿å­˜çš„æ•°æ®
    json_data = {
        "all_metrics": all_metrics,
        "all_history": all_history,
        "timestamps": {
            "round1": timestamp_1,
            "round2": timestamp_2,
        },
        "model_params": {
            "MLP": mlp_params,
            "StdNet": std_params,
            "ResNet": res_params,
        },
        "data_points": {
            "round1": n_points_1,
            "round2": n_points_2,
        },
    }

    # é€’å½’è½¬æ¢æ•°æ®ç»“æ„ä¸­çš„ä¸å¯åºåˆ—åŒ–å¯¹è±¡
    def convert_nested_dict(d):
        for k, v in d.items():
            if isinstance(v, dict):
                convert_nested_dict(v)
            else:
                d[k] = convert_for_json(v)
        return d

    json_data = convert_nested_dict(json_data)

    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
    with open(history_file, "w", encoding="utf-8") as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)

    print(f"\næ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°ç›®å½•: {result_dir}")
    print(f"- æ¨¡å‹æ–‡ä»¶ä¿å­˜åœ¨: {os.path.join(result_dir, 'models')}")
    print(f"- å›¾å½¢æ–‡ä»¶ä¿å­˜åœ¨: {os.path.join(result_dir, 'figures')}")
    print(f"- å¯¹æ¯”æŠ¥å‘Š: {report_file}")
    print(f"- è®­ç»ƒå†å²æ•°æ®: {history_file}")


if __name__ == "__main__":
    main()

# %%
